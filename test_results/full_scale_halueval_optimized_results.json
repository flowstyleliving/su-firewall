{
  "evaluation_type": "full_scale_halueval_optimized",
  "timestamp": "2025-08-17 23:14:42",
  "dataset_size": 10000,
  "optimal_strategy": {
    "name": "conservative",
    "threshold": 0.8,
    "f1": 0.0,
    "precision": 0.0,
    "recall": 0.0,
    "hallucination_rate": 0.0,
    "score": 0.3
  },
  "overall_metrics": {
    "f1_score": 0.0,
    "precision": 0.0,
    "recall": 0.0,
    "auroc": NaN,
    "hallucination_rate": 0.0
  },
  "production_performance": {
    "avg_processing_time_ms": 1.0,
    "throughput_analyses_per_sec": 1000.0,
    "samples_processed": 10000,
    "success_rate": 0.0,
    "concurrent_workers": 8
  },
  "sota_comparison": {
    "benchmarks_beaten": 1,
    "total_benchmarks": 4,
    "production_ready": false,
    "world_class": false
  }
}