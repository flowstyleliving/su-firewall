// üí∞ Token-Based Cost Savings Tracker
// Analyzes token usage and identifies cost optimization opportunities

use crate::SemanticUncertaintyResult;
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, info, instrument};

/// üí≥ Token Cost Models for Different Providers
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenCostModel {
    pub provider: String,
    pub model_name: String,
    pub cost_per_1k_input_tokens: f64,
    pub cost_per_1k_output_tokens: f64,
    pub context_window: u32,
}

impl Default for TokenCostModel {
    fn default() -> Self {
        Self {
            provider: "OpenAI".to_string(),
            model_name: "gpt-4".to_string(),
            cost_per_1k_input_tokens: 0.03,
            cost_per_1k_output_tokens: 0.06,
            context_window: 8192,
        }
    }
}

/// üìä Token Cost Analysis Result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenCostAnalysis {
    pub input_tokens: u32,
    pub output_tokens: u32,
    pub total_tokens: u32,
    pub estimated_cost_usd: f64,
    pub potential_savings_usd: f64,
    pub potential_savings_percent: f64,
    pub optimization_recommendations: Vec<CostOptimizationTip>,
    pub efficiency_score: f64, // 0-1 scale
    pub semantic_quality_cost_ratio: f64, // ‚Ñè‚Çõ per dollar
}

/// üí° Cost Optimization Recommendations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CostOptimizationTip {
    pub category: OptimizationCategory,
    pub description: String,
    pub estimated_savings_percent: f64,
    pub implementation_difficulty: DifficultyLevel,
    pub impact_on_quality: QualityImpact,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum OptimizationCategory {
    PromptCompression,
    TokenEfficiency,
    ModelSelection,
    CachingStrategy,
    BatchProcessing,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DifficultyLevel {
    Easy,
    Medium,
    Hard,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QualityImpact {
    None,
    Minimal,
    Moderate,
    Significant,
}

/// üìà Cost Tracking Statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CostTrackingStats {
    pub total_analyses: u64,
    pub total_tokens_processed: u64,
    pub total_cost_usd: f64,
    pub total_savings_usd: f64,
    pub average_cost_per_analysis: f64,
    pub average_efficiency_score: f64,
    pub top_optimization_categories: Vec<(OptimizationCategory, f64)>,
}

/// üí∞ Cost Tracker Implementation
pub struct CostTracker {
    cost_models: Arc<RwLock<HashMap<String, TokenCostModel>>>,
    stats: Arc<RwLock<CostTrackingStats>>,
    tokenizer: Arc<dyn TokenizerTrait>,
}

/// üî§ Tokenizer Trait for Different Models
pub trait TokenizerTrait: Send + Sync {
    fn count_tokens(&self, text: &str) -> Result<u32>;
    fn estimate_tokens(&self, text: &str) -> u32;
}

/// üìè Simple Tokenizer Implementation
pub struct SimpleTokenizer;

impl TokenizerTrait for SimpleTokenizer {
    fn count_tokens(&self, text: &str) -> Result<u32> {
        // Simple estimation: ~4 characters per token on average
        Ok((text.len() as f64 / 4.0).ceil() as u32)
    }
    
    fn estimate_tokens(&self, text: &str) -> u32 {
        self.count_tokens(text).unwrap_or(0)
    }
}

impl CostTracker {
    /// üöÄ Create new cost tracker
    pub fn new() -> Result<Self> {
        let mut cost_models = HashMap::new();
        
        // Add default models
        cost_models.insert("gpt-4".to_string(), TokenCostModel::default());
        cost_models.insert("gpt-3.5-turbo".to_string(), TokenCostModel {
            provider: "OpenAI".to_string(),
            model_name: "gpt-3.5-turbo".to_string(),
            cost_per_1k_input_tokens: 0.001,
            cost_per_1k_output_tokens: 0.002,
            context_window: 4096,
        });
        cost_models.insert("claude-3-sonnet".to_string(), TokenCostModel {
            provider: "Anthropic".to_string(),
            model_name: "claude-3-sonnet".to_string(),
            cost_per_1k_input_tokens: 0.003,
            cost_per_1k_output_tokens: 0.015,
            context_window: 200000,
        });
        
        Ok(Self {
            cost_models: Arc::new(RwLock::new(cost_models)),
            stats: Arc::new(RwLock::new(CostTrackingStats {
                total_analyses: 0,
                total_tokens_processed: 0,
                total_cost_usd: 0.0,
                total_savings_usd: 0.0,
                average_cost_per_analysis: 0.0,
                average_efficiency_score: 0.0,
                top_optimization_categories: Vec::new(),
            })),
            tokenizer: Arc::new(SimpleTokenizer),
        })
    }
    
    /// üìä Analyze cost for a prompt-output pair
    #[instrument(skip(self, prompt, output, semantic_result))]
    pub async fn analyze_cost(
        &self,
        prompt: &str,
        output: &str,
        semantic_result: &SemanticUncertaintyResult,
    ) -> Result<TokenCostAnalysis> {
        let input_tokens = self.tokenizer.count_tokens(prompt)?;
        let output_tokens = self.tokenizer.count_tokens(output)?;
        let total_tokens = input_tokens + output_tokens;
        
        // Use default model for cost calculation
        let cost_models = self.cost_models.read().await;
        let model = cost_models.get("gpt-4").unwrap();
        
        let estimated_cost_usd = 
            (input_tokens as f64 / 1000.0) * model.cost_per_1k_input_tokens +
            (output_tokens as f64 / 1000.0) * model.cost_per_1k_output_tokens;
        
        // Generate optimization recommendations
        let optimization_recommendations = self.generate_optimization_tips(
            prompt, output, input_tokens, output_tokens, semantic_result
        ).await;
        
        // Calculate potential savings
        let potential_savings_percent = optimization_recommendations.iter()
            .map(|tip| tip.estimated_savings_percent)
            .sum::<f64>();
        
        let potential_savings_usd = estimated_cost_usd * (potential_savings_percent / 100.0);
        
        // Calculate efficiency score
        let efficiency_score = self.calculate_efficiency_score(
            input_tokens, output_tokens, semantic_result
        );
        
        // Calculate semantic quality/cost ratio
        let semantic_quality_cost_ratio = if estimated_cost_usd > 0.0 {
            semantic_result.calibrated_hbar / estimated_cost_usd
        } else {
            0.0
        };
        
        let analysis = TokenCostAnalysis {
            input_tokens,
            output_tokens,
            total_tokens,
            estimated_cost_usd,
            potential_savings_usd,
            potential_savings_percent,
            optimization_recommendations,
            efficiency_score,
            semantic_quality_cost_ratio,
        };
        
        // Update statistics
        self.update_stats(&analysis).await;
        
        info!(
            "Cost analysis: tokens={}, cost=${:.4}, savings=${:.4} ({:.1}%), efficiency={:.2}",
            total_tokens,
            estimated_cost_usd,
            potential_savings_usd,
            potential_savings_percent,
            efficiency_score
        );
        
        Ok(analysis)
    }
    
    /// üí° Generate cost optimization recommendations
    async fn generate_optimization_tips(
        &self,
        prompt: &str,
        output: &str,
        input_tokens: u32,
        output_tokens: u32,
        semantic_result: &SemanticUncertaintyResult,
    ) -> Vec<CostOptimizationTip> {
        let mut tips = Vec::new();
        
        // Check for prompt compression opportunities
        if input_tokens > 500 {
            tips.push(CostOptimizationTip {
                category: OptimizationCategory::PromptCompression,
                description: format!(
                    "Prompt is {} tokens. Consider compression techniques to reduce by 20-30%.",
                    input_tokens
                ),
                estimated_savings_percent: 25.0,
                implementation_difficulty: DifficultyLevel::Easy,
                impact_on_quality: QualityImpact::Minimal,
            });
        }
        
        // Check for repetitive patterns
        if self.detect_repetitive_patterns(prompt) {
            tips.push(CostOptimizationTip {
                category: OptimizationCategory::TokenEfficiency,
                description: "Detected repetitive patterns. Use templates or variables.".to_string(),
                estimated_savings_percent: 15.0,
                implementation_difficulty: DifficultyLevel::Medium,
                impact_on_quality: QualityImpact::None,
            });
        }
        
        // Check for model optimization based on semantic uncertainty
        if semantic_result.calibrated_hbar > 1.5 {
            tips.push(CostOptimizationTip {
                category: OptimizationCategory::ModelSelection,
                description: "High semantic uncertainty detected. Consider using a less expensive model.".to_string(),
                estimated_savings_percent: 70.0,
                implementation_difficulty: DifficultyLevel::Easy,
                impact_on_quality: QualityImpact::Moderate,
            });
        }
        
        // Check for batching opportunities
        if input_tokens < 100 && output_tokens < 100 {
            tips.push(CostOptimizationTip {
                category: OptimizationCategory::BatchProcessing,
                description: "Small request. Consider batching multiple requests for better efficiency.".to_string(),
                estimated_savings_percent: 20.0,
                implementation_difficulty: DifficultyLevel::Medium,
                impact_on_quality: QualityImpact::None,
            });
        }
        
        tips
    }
    
    /// üìä Calculate efficiency score (0-1)
    fn calculate_efficiency_score(
        &self,
        input_tokens: u32,
        output_tokens: u32,
        semantic_result: &SemanticUncertaintyResult,
    ) -> f64 {
        // Higher efficiency = lower tokens, higher semantic quality
        let token_efficiency = 1.0 - (input_tokens as f64 / 1000.0).min(1.0);
        let semantic_quality = semantic_result.calibrated_hbar / 3.0; // Normalize to 0-1
        let output_efficiency = 1.0 - (output_tokens as f64 / 500.0).min(1.0);
        
        (token_efficiency * 0.4 + semantic_quality * 0.4 + output_efficiency * 0.2).max(0.0).min(1.0)
    }
    
    /// üîç Detect repetitive patterns in text
    fn detect_repetitive_patterns(&self, text: &str) -> bool {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut word_counts = HashMap::new();
        
        for word in words {
            *word_counts.entry(word.to_lowercase()).or_insert(0) += 1;
        }
        
        // Check if any word appears more than 3 times
        word_counts.values().any(|&count| count > 3)
    }
    
    /// üìà Update internal statistics
    async fn update_stats(&self, analysis: &TokenCostAnalysis) {
        let mut stats = self.stats.write().await;
        
        stats.total_analyses += 1;
        stats.total_tokens_processed += analysis.total_tokens as u64;
        stats.total_cost_usd += analysis.estimated_cost_usd;
        stats.total_savings_usd += analysis.potential_savings_usd;
        
        // Update averages
        stats.average_cost_per_analysis = stats.total_cost_usd / stats.total_analyses as f64;
        stats.average_efficiency_score = 
            (stats.average_efficiency_score * (stats.total_analyses - 1) as f64 + analysis.efficiency_score) / 
            stats.total_analyses as f64;
    }
    
    /// üí∞ Get total savings achieved
    pub fn get_total_savings(&self) -> f64 {
        // This would be implemented with actual runtime data
        0.0
    }
    
    /// üìä Get cost tracking statistics
    pub async fn get_stats(&self) -> CostTrackingStats {
        self.stats.read().await.clone()
    }
    
    /// üîß Add custom cost model
    pub async fn add_cost_model(&self, model_id: String, model: TokenCostModel) {
        let mut cost_models = self.cost_models.write().await;
        cost_models.insert(model_id, model);
    }
}