# üöÄ Semantic Uncertainty API - User Guide for John Yue

## **üåê API Endpoint**
```
https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev
```

## **üîë Authentication**
Include this header in all requests:
```
X-API-Key: your-production-api-key
```

---

## **üìã Quick Start Examples**

### **1. Health Check**
Test if the API is working:

```bash
curl https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T12:30:45.123Z",
  "version": "1.0.0",
  "engine": "semantic-uncertainty-cloudflare"
}
```

### **2. Analyze Single Prompt**
Get semantic uncertainty for one prompt:

```bash
curl -X POST https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/api/v1/analyze \
  -H 'Content-Type: application/json' \
  -H 'X-API-Key: your-production-api-key' \
  -d '{
    "prompt": "Write a comprehensive guide on AI safety for beginners",
    "model": "gpt4"
  }'
```

**Expected Response:**
```json
{
  "success": true,
  "data": {
    "prompt": "Write a comprehensive guide on AI safety for beginners",
    "model": "gpt4",
    "semantic_uncertainty": 1.2847,
    "precision": 0.3421,
    "flexibility": 0.8976,
    "risk_level": "stable",
    "processing_time": 8,
    "edge_location": "global-edge",
    "timestamp": "2024-01-15T12:30:45.123Z"
  }
}
```

### **3. Batch Analysis**
Analyze multiple prompts at once:

```bash
curl -X POST https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/api/v1/batch \
  -H 'Content-Type: application/json' \
  -H 'X-API-Key: your-production-api-key' \
  -d '{
    "prompts": [
      "Explain quantum computing in simple terms",
      "Write a poem about artificial intelligence",
      "Create a business plan for a tech startup"
    ],
    "model": "claude3"
  }'
```

---

## **üêç Python Examples**

### **Simple Analysis Script**
```python
import requests
import json

API_ENDPOINT = "https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev"
API_KEY = "your-production-api-key"

def analyze_prompt(prompt, model="gpt4"):
    url = f"{API_ENDPOINT}/api/v1/analyze"
    headers = {
        "Content-Type": "application/json",
        "X-API-Key": API_KEY
    }
    data = {
        "prompt": prompt,
        "model": model
    }
    
    response = requests.post(url, headers=headers, json=data)
    return response.json()

# Example usage
result = analyze_prompt("Write about machine learning ethics")
print(f"Semantic Uncertainty: {result['data']['semantic_uncertainty']}")
print(f"Risk Level: {result['data']['risk_level']}")
```

### **Batch Analysis Script**
```python
def batch_analyze(prompts, model="gpt4"):
    url = f"{API_ENDPOINT}/api/v1/batch"
    headers = {
        "Content-Type": "application/json",
        "X-API-Key": API_KEY
    }
    data = {
        "prompts": prompts,
        "model": model
    }
    
    response = requests.post(url, headers=headers, json=data)
    return response.json()

# Example usage
prompts = [
    "Explain neural networks",
    "Write about data privacy",
    "Describe cloud computing"
]

results = batch_analyze(prompts)
for result in results['data']['results']:
    print(f"Prompt: {result['prompt'][:50]}...")
    print(f"H-bar: {result['h_bar']}")
    print(f"Risk: {result['risk_level']}")
    print("---")
```

---

## **üåê JavaScript/Node.js Examples**

### **Simple Analysis**
```javascript
const axios = require('axios');

const API_ENDPOINT = 'https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev';
const API_KEY = 'your-production-api-key';

async function analyzePrompt(prompt, model = 'gpt4') {
    try {
        const response = await axios.post(`${API_ENDPOINT}/api/v1/analyze`, {
            prompt: prompt,
            model: model
        }, {
            headers: {
                'Content-Type': 'application/json',
                'X-API-Key': API_KEY
            }
        });
        
        return response.data;
    } catch (error) {
        console.error('Error analyzing prompt:', error.response?.data || error.message);
        return null;
    }
}

// Example usage
analyzePrompt('Explain quantum computing')
    .then(result => {
        if (result) {
            console.log('Semantic Uncertainty:', result.data.semantic_uncertainty);
            console.log('Risk Level:', result.data.risk_level);
        }
    });
```

### **Browser Fetch Example**
```javascript
async function analyzePrompt(prompt) {
    const response = await fetch('https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/api/v1/analyze', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'X-API-Key': 'your-production-api-key'
        },
        body: JSON.stringify({
            prompt: prompt,
            model: 'gpt4'
        })
    });
    
    const result = await response.json();
    return result;
}
```

---

## **üìä Response Format Guide**

### **Semantic Uncertainty Values**
- **‚Ñè‚Çõ < 1.0**: üî• **High Collapse Risk** - Prompt likely to cause model instability
- **‚Ñè‚Çõ 1.0-1.2**: ‚ö†Ô∏è **Moderate Instability** - Some risk of inconsistent outputs
- **‚Ñè‚Çõ ‚â• 1.2**: ‚úÖ **Stable** - Prompt should produce consistent, reliable outputs

### **Component Breakdown**
- **precision (ŒîŒº)**: Measures variation in model confidence
- **flexibility (ŒîœÉ)**: Measures semantic space exploration
- **h_bar (‚Ñè‚Çõ)**: Overall semantic uncertainty = ‚àö(ŒîŒº √ó ŒîœÉ)

### **Risk Levels**
- `"high_collapse_risk"`: Immediate attention needed
- `"moderate_instability"`: Monitor closely
- `"stable"`: Safe to use

---

## **üéØ Supported Models**

| Model | Code | Stability Factor |
|-------|------|------------------|
| GPT-4 | `gpt4` | 0.85 |
| Claude 3 | `claude3` | 0.82 |
| Gemini | `gemini` | 0.78 |
| Mistral | `mistral` | 0.75 |
| Grok 3 | `grok3` | 0.80 |
| OpenAI o3 | `openai_o3` | 0.90 |

---

## **‚ö° Performance Specs**

- **Latency**: Sub-10ms globally (300+ edge locations)
- **Throughput**: 10,000+ requests/second
- **Availability**: 99.99% uptime
- **Rate Limit**: 100 requests/minute (current setting)
- **Max Prompt Length**: 10,000 characters
- **Batch Size**: Up to 50 prompts per request

---

## **üö® Error Handling**

### **Common Error Responses**

**401 Unauthorized:**
```json
{
  "error": "Unauthorized",
  "message": "Valid API key required"
}
```

**400 Bad Request:**
```json
{
  "error": "Bad Request",
  "message": "Prompt is required"
}
```

**429 Rate Limited:**
```json
{
  "error": "Rate limit exceeded",
  "limit": 100,
  "reset_in": 60
}
```

### **Error Handling in Code**
```python
try:
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()  # Raises exception for HTTP errors
    result = response.json()
    
    if not result.get('success'):
        print(f"API Error: {result.get('error', 'Unknown error')}")
        return None
        
    return result['data']
    
except requests.exceptions.HTTPError as e:
    print(f"HTTP Error: {e}")
except requests.exceptions.RequestException as e:
    print(f"Request Error: {e}")
```

---

## **üí° Best Practices**

### **1. Optimize Your Prompts**
- **Shorter prompts**: Generally more stable (lower ‚Ñè‚Çõ)
- **Clear instructions**: Reduce ambiguity
- **Specific context**: Include relevant details

### **2. Batch When Possible**
- **More efficient**: Single request for multiple prompts
- **Better insights**: Compare ‚Ñè‚Çõ values across prompts
- **Cost effective**: Reduces API call overhead

### **3. Monitor Risk Levels**
```python
def should_use_prompt(h_bar):
    if h_bar < 1.0:
        return False, "High collapse risk - avoid this prompt"
    elif h_bar < 1.2:
        return True, "Moderate risk - monitor output quality"
    else:
        return True, "Safe to use"

safe, message = should_use_prompt(result['data']['semantic_uncertainty'])
print(message)
```

### **4. Handle Rate Limits**
```python
import time

def safe_api_call(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = analyze_prompt(prompt)
            return result
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:  # Rate limited
                wait_time = 60  # Wait 1 minute
                print(f"Rate limited. Waiting {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                raise
    
    raise Exception("Max retries exceeded")
```

---

## **üîß Testing Your Integration**

### **1. Test Health Endpoint**
```bash
curl https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/health
```

### **2. Test Simple Analysis**
```bash
curl -X POST https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/api/v1/analyze \
  -H 'Content-Type: application/json' \
  -H 'X-API-Key: your-production-api-key' \
  -d '{"prompt": "Hello world", "model": "gpt4"}'
```

### **3. Test Error Handling**
```bash
# Test without API key
curl -X POST https://semantic-uncertainty-api-production.semantic-uncertainty.workers.dev/api/v1/analyze \
  -H 'Content-Type: application/json' \
  -d '{"prompt": "Test"}'
```

---

## **üìû Support & Feedback**

- **Issues**: Report any problems or unexpected behavior
- **Feature Requests**: Suggest new models or capabilities
- **Performance**: Share latency measurements from your location
- **Use Cases**: Let us know how you're using the API

---

## **üöÄ Ready to Start!**

The API is live and ready for testing. Start with the health check, then try analyzing a simple prompt. The semantic uncertainty values will help you identify which prompts are safe to use with language models.

**Happy testing, John!** üéØ 