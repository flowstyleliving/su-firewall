{
  "evaluation_summary": {
    "total_evaluations": 84,
    "methods_tested": [
      "diag_fim_dir",
      "scalar_js_kl",
      "scalar_trace"
    ],
    "models_tested": [
      "mistral-7b",
      "mixtral-8x7b"
    ],
    "datasets_tested": [
      "truthfulqa"
    ],
    "evaluation_timestamp": "2025-08-15 15:03:21"
  },
  "overall_performance": {
    "accuracy": 0.5952380952380952,
    "precision": 0.7222222222222222,
    "recall": 0.30952380952380953,
    "f1_score": 0.4333333333333334,
    "avg_processing_time_ms": 3.7563698632376537,
    "total_samples": 84
  },
  "performance_by_method": {
    "diag_fim_dir": {
      "accuracy": 0.5172413793103449,
      "precision": 0.6666666666666666,
      "recall": 0.25,
      "f1_score": 0.36363636363636365,
      "avg_processing_time_ms": 4.516050733368973,
      "total_samples": 29
    },
    "scalar_js_kl": {
      "accuracy": 0.6428571428571429,
      "precision": 0.7272727272727273,
      "recall": 0.5333333333333333,
      "f1_score": 0.6153846153846153,
      "avg_processing_time_ms": 3.4454550061907088,
      "total_samples": 28
    },
    "scalar_trace": {
      "accuracy": 0.6296296296296297,
      "precision": 1.0,
      "recall": 0.09090909090909091,
      "f1_score": 0.16666666666666669,
      "avg_processing_time_ms": 3.262846558182328,
      "total_samples": 27
    }
  },
  "performance_by_dataset": {
    "truthfulqa": {
      "accuracy": 0.5952380952380952,
      "precision": 0.7222222222222222,
      "recall": 0.30952380952380953,
      "f1_score": 0.4333333333333334,
      "avg_processing_time_ms": 3.7563698632376537,
      "total_samples": 84
    }
  },
  "performance_by_model": {
    "mistral-7b": {
      "accuracy": 0.5714285714285714,
      "precision": 0.6666666666666666,
      "recall": 0.2857142857142857,
      "f1_score": 0.4,
      "avg_processing_time_ms": 3.291260628473191,
      "total_samples": 42
    },
    "mixtral-8x7b": {
      "accuracy": 0.6190476190476191,
      "precision": 0.7777777777777778,
      "recall": 0.3333333333333333,
      "f1_score": 0.4666666666666666,
      "avg_processing_time_ms": 4.221479098002116,
      "total_samples": 42
    }
  },
  "insights": {
    "hbar_fep_correlation": -0.038880796477115766,
    "performance_by_type": {
      "factual_accuracy": 0.21428571428571427,
      "conversational_accuracy": 0.0
    },
    "processing_time_by_method": {
      "diag_fim_dir": 4.516050733368973,
      "scalar_js_kl": 3.4454550061907088,
      "scalar_trace": 3.262846558182328
    }
  }
}