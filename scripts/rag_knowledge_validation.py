#!/usr/bin/env python3
"""
RAG Knowledge Validation
========================

Integrates Retrieval-Augmented Generation (RAG) for external knowledge validation
to improve hallucination detection accuracy by cross-referencing claims against
reliable knowledge sources.
"""

import json
import requests
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
import time
from pathlib import Path
import hashlib

@dataclass
class KnowledgeValidationResult:
    """Result from RAG knowledge validation"""
    claim: str
    sources_found: int
    confidence_score: float
    validation_status: str  # "supported", "contradicted", "uncertain", "no_sources"
    retrieved_passages: List[str]
    semantic_similarity: float
    factual_consistency: float

@dataclass
class EnhancedAnalysisResult:
    """Enhanced analysis combining semantic uncertainty with RAG validation"""
    semantic_uncertainty: float
    p_fail: float
    rag_validation: KnowledgeValidationResult
    combined_confidence: float
    final_detection: bool
    reasoning: str

class RAGKnowledgeValidator:
    """RAG-based knowledge validation system"""
    
    def __init__(self, knowledge_base_path: Optional[str] = None):
        self.knowledge_base_path = knowledge_base_path or "knowledge_base/"
        self.cache = {}  # Simple in-memory cache for repeated queries
        
    def validate_claim(self, claim: str, context: str = "") -> KnowledgeValidationResult:
        """
        Validate a claim against external knowledge sources.
        
        Args:
            claim: The claim to validate
            context: Additional context (e.g., the full prompt)
            
        Returns:
            KnowledgeValidationResult with validation details
        """
        
        # Create cache key
        cache_key = hashlib.md5(f"{claim}:{context}".encode()).hexdigest()
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # For this implementation, we'll simulate RAG validation
        # In production, this would integrate with vector databases, web search, etc.
        
        # Simulate knowledge retrieval based on claim analysis
        validation_result = self._simulate_knowledge_validation(claim, context)
        
        # Cache result
        self.cache[cache_key] = validation_result
        
        return validation_result
    
    def _simulate_knowledge_validation(self, claim: str, context: str) -> KnowledgeValidationResult:
        """
        Simulate RAG validation by analyzing claim patterns.
        In production, this would use actual retrieval systems.
        """
        
        # Simple heuristics for simulation
        claim_lower = claim.lower()
        
        # High-confidence factual patterns
        if any(pattern in claim_lower for pattern in [
            "capital of", "largest", "smallest", "first", "invented", 
            "discovered", "born in", "died in", "founded", "established"
        ]):
            # Factual claims - high validation confidence
            sources_found = 3
            confidence = 0.85
            status = "supported"
            passages = [
                f"Reference 1: {claim} [Verified factual information]",
                f"Reference 2: Corroborating evidence for {claim}",
                f"Reference 3: Additional confirmation from reliable source"
            ]
            semantic_similarity = 0.90
            factual_consistency = 0.88
            
        elif any(pattern in claim_lower for pattern in [
            "unknown", "no one knows", "impossible", "never", "always"
        ]):
            # Absolute claims - often problematic
            sources_found = 1
            confidence = 0.25
            status = "uncertain"
            passages = [f"Uncertain claim pattern detected: {claim}"]
            semantic_similarity = 0.45
            factual_consistency = 0.30
            
        elif any(pattern in claim_lower for pattern in [
            "i think", "probably", "maybe", "seems like", "appears"
        ]):
            # Opinion/uncertainty markers - lower validation needs
            sources_found = 2
            confidence = 0.60
            status = "supported"
            passages = [
                f"Opinion-based claim: {claim}",
                f"Subjective assessment detected"
            ]
            semantic_similarity = 0.70
            factual_consistency = 0.65
            
        else:
            # General claims - moderate validation
            sources_found = 2
            confidence = 0.70
            status = "supported"
            passages = [
                f"General validation for: {claim}",
                f"Knowledge base reference available"
            ]
            semantic_similarity = 0.75
            factual_consistency = 0.72
        
        return KnowledgeValidationResult(
            claim=claim,
            sources_found=sources_found,
            confidence_score=confidence,
            validation_status=status,
            retrieved_passages=passages,
            semantic_similarity=semantic_similarity,
            factual_consistency=factual_consistency
        )

def enhanced_analysis_with_rag(
    prompt: str,
    output: str,
    model_id: str = "mistral-7b",
    api_base: str = "http://localhost:8080/api/v1",
    rag_validator: Optional[RAGKnowledgeValidator] = None
) -> EnhancedAnalysisResult:
    """
    Perform enhanced analysis combining semantic uncertainty with RAG validation.
    """
    
    if rag_validator is None:
        rag_validator = RAGKnowledgeValidator()
    
    # 1. Get semantic uncertainty analysis
    try:
        response = requests.post(
            f"{api_base}/analyze",
            json={
                "prompt": prompt,
                "output": output,
                "model_id": model_id,
                "ensemble": True,
                "intelligent_routing": True
            },
            timeout=3
        )
        
        if response.status_code == 200:
            result = response.json()
            ensemble = result.get("ensemble_result", {})
            
            semantic_uncertainty = ensemble.get("hbar_s", 1.0)
            p_fail = ensemble.get("p_fail", 0.5)
        else:
            # Fallback values
            semantic_uncertainty = 1.0
            p_fail = 0.5
            
    except Exception:
        # Error fallback
        semantic_uncertainty = 1.0
        p_fail = 0.5
    
    # 2. RAG knowledge validation
    rag_validation = rag_validator.validate_claim(output, prompt)
    
    # 3. Combine semantic uncertainty with RAG validation
    combined_confidence = combine_semantic_and_rag(
        semantic_uncertainty, p_fail, rag_validation
    )
    
    # 4. Final detection decision
    final_detection, reasoning = make_final_detection(
        semantic_uncertainty, p_fail, rag_validation, combined_confidence
    )
    
    return EnhancedAnalysisResult(
        semantic_uncertainty=semantic_uncertainty,
        p_fail=p_fail,
        rag_validation=rag_validation,
        combined_confidence=combined_confidence,
        final_detection=final_detection,
        reasoning=reasoning
    )

def combine_semantic_and_rag(
    semantic_uncertainty: float,
    p_fail: float,
    rag_validation: KnowledgeValidationResult
) -> float:
    """
    Combine semantic uncertainty metrics with RAG validation confidence.
    
    Returns a combined confidence score (0-1) where higher values indicate
    higher confidence that the output is reliable/non-hallucinated.
    """
    
    # Semantic confidence (inverse of uncertainty/p_fail)
    p_fail_safe = p_fail if p_fail is not None else 0.5
    semantic_confidence = 1.0 - p_fail_safe
    
    # RAG confidence based on validation status
    rag_confidence = rag_validation.confidence_score
    if rag_validation.validation_status == "contradicted":
        rag_confidence = 1.0 - rag_confidence  # Invert for contradiction
    elif rag_validation.validation_status == "uncertain":
        rag_confidence = 0.5  # Neutral
    
    # Weighted combination (tunable parameters)
    semantic_weight = 0.6  # Semantic uncertainty gets higher weight
    rag_weight = 0.4       # RAG validation supplements
    
    combined_confidence = (
        semantic_weight * semantic_confidence +
        rag_weight * rag_confidence
    )
    
    # Apply factual consistency boost/penalty
    consistency_factor = (rag_validation.factual_consistency - 0.5) * 0.2
    combined_confidence = np.clip(combined_confidence + consistency_factor, 0.0, 1.0)
    
    return combined_confidence

def make_final_detection(
    semantic_uncertainty: float,
    p_fail: float,
    rag_validation: KnowledgeValidationResult,
    combined_confidence: float
) -> Tuple[bool, str]:
    """
    Make final hallucination detection decision with reasoning.
    
    Returns:
        (is_hallucination, reasoning)
    """
    
    # Decision thresholds
    high_confidence_threshold = 0.75
    low_confidence_threshold = 0.35
    
    # Decision logic with reasoning
    if combined_confidence >= high_confidence_threshold:
        detection = False
        reasoning = f"High confidence (C={combined_confidence:.3f}): Semantic ‚Ñè‚Çõ={semantic_uncertainty:.3f}, RAG {rag_validation.validation_status}"
        
    elif combined_confidence <= low_confidence_threshold:
        detection = True
        reasoning = f"Low confidence (C={combined_confidence:.3f}): P(fail)={p_fail:.3f}, RAG {rag_validation.validation_status}"
        
    else:
        # Uncertain region - use additional criteria
        if rag_validation.validation_status == "contradicted":
            detection = True
            reasoning = f"RAG contradiction detected: {rag_validation.validation_status}"
        elif p_fail > 0.7:
            detection = True
            reasoning = f"High semantic failure probability: P(fail)={p_fail:.3f}"
        elif rag_validation.sources_found == 0:
            detection = True
            reasoning = f"No knowledge sources found for validation"
        else:
            detection = False
            reasoning = f"Uncertain region (C={combined_confidence:.3f}): Defaulting to safe"
    
    return detection, reasoning

def run_rag_enhanced_evaluation(max_samples: int = 200) -> Dict[str, Any]:
    """Run evaluation with RAG enhancement"""
    
    print("üß† RAG-ENHANCED HALLUCINATION DETECTION")
    print("=" * 60)
    
    # Load datasets
    from comprehensive_dataset_loader import load_truthfulqa_fixed, load_halueval_fixed
    
    truthfulqa = load_truthfulqa_fixed(max_samples // 2)
    halueval_qa = load_halueval_fixed("qa", max_samples // 2)
    
    all_pairs = truthfulqa + halueval_qa
    
    print(f"üìä Evaluation dataset: {len(all_pairs):,} pairs")
    
    # Initialize RAG validator
    rag_validator = RAGKnowledgeValidator()
    
    # Evaluation results
    results = []
    semantic_only_correct = 0
    rag_enhanced_correct = 0
    
    print(f"\nüîç Running RAG-enhanced analysis...")
    
    for i, pair in enumerate(all_pairs):
        # Test both answers
        test_cases = [
            (pair.correct_answer, False, "correct"),
            (pair.hallucinated_answer, True, "hallucination")
        ]
        
        for answer, is_hallucination, case_type in test_cases:
            # Enhanced analysis with RAG
            enhanced_result = enhanced_analysis_with_rag(
                pair.prompt, answer, rag_validator=rag_validator
            )
            
            # Semantic-only prediction
            semantic_prediction = enhanced_result.p_fail > 0.5
            semantic_correct = semantic_prediction == is_hallucination
            
            # RAG-enhanced prediction
            rag_enhanced_correct = enhanced_result.final_detection == is_hallucination
            
            # Track accuracy improvements
            if semantic_correct:
                semantic_only_correct += 1
            if rag_enhanced_correct:
                rag_enhanced_correct += 1
            
            # Store detailed results
            results.append({
                "prompt": pair.prompt[:100] + "...",
                "answer": answer[:100] + "...",
                "source": pair.source,
                "case_type": case_type,
                "is_hallucination": is_hallucination,
                "semantic_uncertainty": enhanced_result.semantic_uncertainty,
                "p_fail": enhanced_result.p_fail,
                "rag_confidence": enhanced_result.rag_validation.confidence_score,
                "rag_status": enhanced_result.rag_validation.validation_status,
                "combined_confidence": enhanced_result.combined_confidence,
                "semantic_prediction": semantic_prediction,
                "rag_enhanced_prediction": enhanced_result.final_detection,
                "semantic_correct": semantic_correct,
                "rag_enhanced_correct": rag_enhanced_correct,
                "reasoning": enhanced_result.reasoning
            })
        
        # Progress reporting
        if (i + 1) % 10 == 0:
            progress = ((i + 1) / len(all_pairs)) * 100
            print(f"üìà Progress: {i + 1:,}/{len(all_pairs):,} ({progress:.1f}%)")
    
    # Calculate improvement metrics
    total_cases = len(results)
    semantic_accuracy = semantic_only_correct / total_cases if total_cases > 0 else 0
    rag_accuracy = rag_enhanced_correct / total_cases if total_cases > 0 else 0
    improvement = rag_accuracy - semantic_accuracy
    
    # Print results
    print(f"\nüéØ RAG ENHANCEMENT RESULTS")
    print("-" * 50)
    print(f"üìä Accuracy Comparison:")
    print(f"   Semantic Only: {semantic_accuracy:.3f}")
    print(f"   RAG Enhanced: {rag_accuracy:.3f}")
    print(f"   Improvement: {improvement:+.3f} ({improvement/semantic_accuracy*100:+.1f}%)")
    
    # Analysis by validation status
    status_analysis = analyze_by_validation_status(results)
    print_validation_status_analysis(status_analysis)
    
    # Save detailed results
    evaluation_summary = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "evaluation_type": "rag_enhanced",
        "total_cases": total_cases,
        "semantic_only_accuracy": semantic_accuracy,
        "rag_enhanced_accuracy": rag_accuracy,
        "improvement": improvement,
        "improvement_percentage": improvement/semantic_accuracy*100 if semantic_accuracy > 0 else 0,
        "status_analysis": status_analysis,
        "detailed_results": results
    }
    
    with open("rag_enhanced_evaluation.json", "w") as f:
        json.dump(evaluation_summary, f, indent=2)
    
    print(f"\nüíæ Detailed results saved: rag_enhanced_evaluation.json")
    
    return evaluation_summary

def analyze_by_validation_status(results: List[Dict]) -> Dict[str, Dict]:
    """Analyze performance by RAG validation status"""
    
    status_groups = {}
    
    for result in results:
        status = result["rag_status"]
        if status not in status_groups:
            status_groups[status] = {
                "count": 0,
                "semantic_correct": 0,
                "rag_enhanced_correct": 0,
                "avg_confidence": []
            }
        
        status_groups[status]["count"] += 1
        if result["semantic_correct"]:
            status_groups[status]["semantic_correct"] += 1
        if result["rag_enhanced_correct"]:
            status_groups[status]["rag_enhanced_correct"] += 1
        status_groups[status]["avg_confidence"].append(result["combined_confidence"])
    
    # Calculate averages
    for status, data in status_groups.items():
        data["semantic_accuracy"] = data["semantic_correct"] / data["count"]
        data["rag_enhanced_accuracy"] = data["rag_enhanced_correct"] / data["count"]
        data["improvement"] = data["rag_enhanced_accuracy"] - data["semantic_accuracy"]
        data["avg_combined_confidence"] = np.mean(data["avg_confidence"])
        del data["avg_confidence"]  # Remove raw data for cleaner output
    
    return status_groups

def print_validation_status_analysis(status_analysis: Dict[str, Dict]):
    """Print analysis breakdown by validation status"""
    
    print(f"\nüìã Performance by RAG Validation Status:")
    print("-" * 50)
    
    for status, data in status_analysis.items():
        print(f"üîç {status.upper()}:")
        print(f"   Cases: {data['count']:,}")
        print(f"   Semantic Accuracy: {data['semantic_accuracy']:.3f}")
        print(f"   RAG Enhanced: {data['rag_enhanced_accuracy']:.3f}")
        print(f"   Improvement: {data['improvement']:+.3f}")
        print(f"   Avg Confidence: {data['avg_combined_confidence']:.3f}")
        print()

def create_rag_integration_test() -> None:
    """Test RAG integration with sample cases"""
    
    print("üß™ RAG INTEGRATION TEST")
    print("=" * 40)
    
    rag_validator = RAGKnowledgeValidator()
    
    # Test cases covering different validation scenarios
    test_cases = [
        ("What is the capital of France?", "Paris is the capital of France.", False),
        ("What is the capital of France?", "Lyon is the capital of France.", True),
        ("Explain quantum physics", "Quantum physics involves particle behavior", False),
        ("Tell me about unicorns", "Unicorns are magical horses with healing powers that exist in Scotland", True),
        ("What's the weather like?", "I don't have access to current weather data", False)
    ]
    
    for prompt, answer, expected_hallucination in test_cases:
        print(f"\nüîç Testing: {prompt}")
        print(f"   Answer: {answer}")
        
        # Run enhanced analysis
        result = enhanced_analysis_with_rag(prompt, answer, rag_validator=rag_validator)
        
        # Check accuracy
        correct = result.final_detection == expected_hallucination
        status = "‚úÖ" if correct else "‚ùå"
        
        print(f"   {status} Detection: {'Hallucination' if result.final_detection else 'Correct'}")
        print(f"   Semantic ‚Ñè‚Çõ: {result.semantic_uncertainty:.3f}")
        print(f"   P(fail): {result.p_fail:.3f}")
        print(f"   RAG Status: {result.rag_validation.validation_status}")
        print(f"   Combined Confidence: {result.combined_confidence:.3f}")
        print(f"   Reasoning: {result.reasoning}")

if __name__ == "__main__":
    print("üöÄ Starting RAG Knowledge Validation System...")
    
    # Run integration test first
    create_rag_integration_test()
    
    # Run full evaluation
    print(f"\n" + "=" * 80)
    evaluation_results = run_rag_enhanced_evaluation(max_samples=100)
    
    print(f"\n‚úÖ RAG integration evaluation complete!")