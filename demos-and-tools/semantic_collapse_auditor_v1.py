#!/usr/bin/env python3
"""
üöÄ SEMANTIC COLLAPSE AUDITOR V1
===============================

The first zero-shot collapse audit tool for foundation model safety.

Target: Research labs, OSS model developers, AI safety teams

Features:
- ‚úÖ ROC curve analysis for threshold optimization
- ‚úÖ Model-specific calibration
- ‚úÖ Failure mode segmentation (hallucination, jailbreak, semantic drift)
- ‚úÖ Risk assessment (low, medium, high, critical)
- ‚úÖ Multi-dataset validation
- ‚úÖ Enterprise-ready reporting

Usage:
    python demos-and-tools/semantic_collapse_auditor_v1.py --model llama3-70b
    python demos-and-tools/semantic_collapse_auditor_v1.py --benchmark full --export-report
"""

import asyncio
import argparse
import sys
import time
from pathlib import Path
from typing import Dict, List
import json

# Add evaluation-frameworks to path
sys.path.append(str(Path(__file__).parent.parent / "evaluation-frameworks"))

from semantic_collapse_validation import SemanticCollapseValidator

class SemanticCollapseAuditorV1:
    """Commercial-grade semantic collapse auditing tool"""
    
    def __init__(self, target_model: str = None, benchmark_level: str = "standard"):
        self.target_model = target_model
        self.benchmark_level = benchmark_level
        self.auditor = SemanticCollapseValidator(save_results=True)
        
        # Configure for target use case
        self._configure_for_research_labs()
    
    def _configure_for_research_labs(self):
        """Configure settings optimized for research lab usage"""
        if self.benchmark_level == "quick":
            # Quick audit for development iterations
            self.auditor.models = ['gpt4', 'claude3'] if not self.target_model else [self.target_model]
        elif self.benchmark_level == "standard":
            # Standard audit for model evaluation
            self.auditor.models = ['gpt4', 'claude3', 'gemini', 'mistral'] if not self.target_model else [self.target_model]
        elif self.benchmark_level == "comprehensive":
            # Full audit for research publication
            self.auditor.models = ['gpt4', 'claude3', 'gemini', 'gemini_flash', 'mistral'] if not self.target_model else [self.target_model]
        
        # Update output directory for professional reporting
        self.auditor.output_dir = Path("semantic_collapse_audit_results")
        self.auditor.output_dir.mkdir(exist_ok=True)
    
    def print_commercial_header(self):
        """Print professional audit header"""
        print("üöÄ SEMANTIC COLLAPSE AUDITOR V1")
        print("=" * 60)
        print("üî¨ Zero-shot collapse detection for foundation models")
        print("üéØ Research lab & OSS model safety validation")
        print("üìä Enterprise-grade semantic uncertainty analysis")
        print("=" * 60)
        print(f"üßÆ Equation: ‚Ñè‚Çõ(C) = ‚àö(ŒîŒº √ó ŒîœÉ)")
        print(f"‚ö° Benchmark Level: {self.benchmark_level}")
        if self.target_model:
            print(f"ü§ñ Target Model: {self.target_model}")
        else:
            print(f"ü§ñ Model Suite: {', '.join(self.auditor.models)}")
        print("=" * 60)
    
    async def run_audit(self) -> Dict:
        """Run complete semantic collapse audit"""
        start_time = time.time()
        
        # Print header
        self.print_commercial_header()
        
        # Run validation
        print("\nüîç RUNNING SEMANTIC COLLAPSE AUDIT...")
        await self.auditor.run_validation()
        
        # Display results
        print("\nüìä AUDIT RESULTS")
        print("=" * 60)
        self.auditor.display_results()
        
        # Save comprehensive results
        print("\nüíæ GENERATING AUDIT REPORT...")
        try:
            self.auditor.save_results()
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error saving results: {e}")
        
        audit_time = time.time() - start_time
        
        # Generate audit summary
        summary = self._generate_audit_summary(audit_time)
        
        return summary
    
    def _generate_audit_summary(self, audit_time: float) -> Dict:
        """Generate executive summary for audit"""
        results_df = self.auditor.validation_results
        
        if not results_df:
            return {"status": "error", "message": "No results generated"}
        
        # Convert to DataFrame for analysis
        import pandas as pd
        df = pd.DataFrame([{
            'model': r.model,
            'dataset': r.dataset,
            'failure_mode': r.failure_mode,
            'risk_level': r.risk_level,
            'hbar_s': r.hbar_s,
            'prediction_correct': r.prediction_correct,
            'known_failure': r.known_failure
        } for r in results_df])
        
        # Calculate key metrics
        overall_accuracy = df['prediction_correct'].mean()
        total_evaluations = len(df)
        
        # Risk distribution
        risk_counts = df['risk_level'].value_counts()
        
        # Failure mode performance
        failure_mode_accuracy = df.groupby('failure_mode')['prediction_correct'].mean()
        
        # Model performance (if multiple models)
        model_performance = df.groupby('model')['prediction_correct'].mean()
        
        summary = {
            "audit_metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "audit_duration_seconds": round(audit_time, 2),
                "benchmark_level": self.benchmark_level,
                "total_evaluations": total_evaluations,
                "models_tested": list(df['model'].unique()),
                "datasets_covered": list(df['dataset'].unique())
            },
            "key_findings": {
                "overall_accuracy": round(overall_accuracy, 3),
                "risk_distribution": {
                    "critical": int(risk_counts.get('critical', 0)),
                    "high": int(risk_counts.get('high', 0)),
                    "medium": int(risk_counts.get('medium', 0)),
                    "low": int(risk_counts.get('low', 0))
                },
                "failure_mode_accuracy": {
                    mode: round(acc, 3) for mode, acc in failure_mode_accuracy.items()
                },
                "model_performance": {
                    model: round(acc, 3) for model, acc in model_performance.items()
                }
            },
            "recommendations": self._generate_recommendations(df),
            "next_steps": [
                "üîß Calibrate thresholds using model-specific optima",
                "üìä Deploy ROC-optimized detection in production",
                "üß™ Focus on failure modes with <80% accuracy",
                "‚öôÔ∏è Implement model-specific risk assessment"
            ]
        }
        
        return summary
    
    def _generate_recommendations(self, df) -> List[str]:
        """Generate actionable recommendations based on audit results"""
        recommendations = []
        
        overall_accuracy = df['prediction_correct'].mean()
        
        # Overall performance assessment
        if overall_accuracy > 0.85:
            recommendations.append("‚úÖ EXCELLENT: ‚Ñè‚Çõ equation provides reliable collapse detection")
            recommendations.append("üéØ DEPLOY: Ready for production safety monitoring")
        elif overall_accuracy > 0.70:
            recommendations.append("üü° GOOD: ‚Ñè‚Çõ shows promising collapse detection capability")
            recommendations.append("üîß TUNE: Optimize thresholds using ROC analysis")
        else:
            recommendations.append("üî¥ CAUTION: ‚Ñè‚Çõ requires calibration before deployment")
            recommendations.append("üìä ANALYZE: Focus on failure mode segmentation")
        
        # Risk level analysis
        critical_count = len(df[df['risk_level'] == 'critical'])
        if critical_count > 0:
            recommendations.append(f"‚ö†Ô∏è CRITICAL: {critical_count} critical risk cases detected")
            recommendations.append("üö® IMMEDIATE: Review and address critical failure modes")
        
        # Model-specific recommendations
        if len(df['model'].unique()) > 1:
            worst_model = df.groupby('model')['prediction_correct'].mean().idxmin()
            recommendations.append(f"ü§ñ FOCUS: {worst_model} shows lowest detection accuracy")
            recommendations.append("‚öôÔ∏è CALIBRATE: Implement model-specific thresholds")
        
        # Failure mode recommendations
        failure_mode_accuracy = df.groupby('failure_mode')['prediction_correct'].mean()
        worst_failure_mode = failure_mode_accuracy.idxmin()
        worst_accuracy = failure_mode_accuracy[worst_failure_mode]
        
        if worst_accuracy < 0.7:
            recommendations.append(f"üß© INVESTIGATE: {worst_failure_mode.replace('_', ' ')} detection needs improvement")
            recommendations.append("üìö ENHANCE: Add more training examples for weak failure modes")
        
        return recommendations
    
    def print_audit_summary(self, summary: Dict):
        """Print executive summary of audit results"""
        print("\n" + "=" * 80)
        print("üìã SEMANTIC COLLAPSE AUDIT SUMMARY")
        print("=" * 80)
        
        metadata = summary['audit_metadata']
        findings = summary['key_findings']
        
        print(f"‚è±Ô∏è  Audit Duration: {metadata['audit_duration_seconds']}s")
        print(f"üß™ Total Evaluations: {metadata['total_evaluations']}")
        print(f"ü§ñ Models Tested: {', '.join(metadata['models_tested'])}")
        print(f"üìö Datasets: {', '.join(metadata['datasets_covered'])}")
        
        print(f"\nüéØ KEY FINDINGS:")
        print(f"   Overall Accuracy: {findings['overall_accuracy']:.1%}")
        
        risk_dist = findings['risk_distribution']
        print(f"   Risk Distribution:")
        print(f"     üî¥ Critical: {risk_dist['critical']}")
        print(f"     üü† High: {risk_dist['high']}")
        print(f"     üü° Medium: {risk_dist['medium']}")
        print(f"     üü¢ Low: {risk_dist['low']}")
        
        print(f"\nüß© FAILURE MODE PERFORMANCE:")
        for mode, accuracy in findings['failure_mode_accuracy'].items():
            status = "‚úÖ" if accuracy > 0.8 else "‚ö†Ô∏è" if accuracy > 0.6 else "‚ùå"
            print(f"   {status} {mode.replace('_', ' ').title()}: {accuracy:.1%}")
        
        if len(findings['model_performance']) > 1:
            print(f"\nü§ñ MODEL PERFORMANCE:")
            for model, accuracy in findings['model_performance'].items():
                status = "‚úÖ" if accuracy > 0.8 else "‚ö†Ô∏è" if accuracy > 0.6 else "‚ùå"
                print(f"   {status} {model}: {accuracy:.1%}")
        
        print(f"\nüí° RECOMMENDATIONS:")
        for rec in summary['recommendations']:
            print(f"   {rec}")
        
        print(f"\nüöÄ NEXT STEPS:")
        for step in summary['next_steps']:
            print(f"   {step}")
        
        print("\n" + "=" * 80)
        print("üéØ AUDIT COMPLETE - Results saved to semantic_collapse_audit_results/")
        print("üìä ROC analysis: semantic_collapse_audit_results/roc_analysis.png")
        print("üìã Full report: semantic_collapse_audit_results/validation_summary.json")
        print("=" * 80)

def main():
    """Main CLI interface"""
    parser = argparse.ArgumentParser(description="Semantic Collapse Auditor V1")
    parser.add_argument('--model', type=str, help='Target model to audit (e.g., llama3-70b)')
    parser.add_argument('--benchmark', choices=['quick', 'standard', 'comprehensive'], 
                        default='standard', help='Benchmark level')
    parser.add_argument('--export-report', action='store_true', 
                        help='Export detailed audit report')
    
    args = parser.parse_args()
    
    # Initialize auditor
    auditor = SemanticCollapseAuditorV1(
        target_model=args.model,
        benchmark_level=args.benchmark
    )
    
    async def run_audit():
        # Run audit
        summary = await auditor.run_audit()
        
        # Print summary
        auditor.print_audit_summary(summary)
        
        # Export report if requested
        if args.export_report:
            report_path = auditor.auditor.output_dir / "executive_summary.json"
            with open(report_path, 'w') as f:
                json.dump(summary, f, indent=2)
            print(f"üìã Executive summary exported to {report_path}")
        
        # Commercial positioning
        print("\nüåü SEMANTIC COLLAPSE AUDITOR V1")
        print("   The first zero-shot collapse audit tool for foundation model safety")
        print("   Perfect for:")
        print("     üî¨ Research labs validating new models")
        print("     üè¢ Enterprise teams deploying OSS models")
        print("     üõ°Ô∏è AI safety teams auditing model behavior")
        print("     üìä Model developers optimizing safety thresholds")
    
    # Run the audit
    asyncio.run(run_audit())

if __name__ == "__main__":
    main() 