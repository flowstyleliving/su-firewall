#!/usr/bin/env python3
"""
Universal Semantic Collapse Test - Detailed Analysis
===================================================

Analyze the detailed responses from the universal test.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
import os

def analyze_responses():
    # Find the latest results file
    results_files = [f for f in os.listdir('.') if f.startswith('universal_semantic_test_') and f.endswith('.csv')]
    if not results_files:
        print("‚ùå No results files found. Run universal_semantic_test.py first.")
        return
    
    latest_file = sorted(results_files)[-1]
    print(f"üìä Analyzing results from: {latest_file}")
    
    # Load results
    df = pd.read_csv(latest_file)
    
    print("\n" + "=" * 80)
    print("üî¨ DETAILED UNIVERSAL SEMANTIC COLLAPSE ANALYSIS")
    print("=" * 80)
    
    # Create comprehensive analysis visualization
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Universal Semantic Collapse Test - Comprehensive Analysis', fontsize=16, fontweight='bold')
    
    # 1. ‚Ñè‚Çõ scores by model
    ax1 = axes[0, 0]
    colors = ['#ff4444' if x else '#44ff44' for x in df['collapse_risk']]
    bars = ax1.bar(range(len(df)), df['hbar_s'], color=colors, alpha=0.7)
    ax1.set_title('Semantic Uncertainty (‚Ñè‚Çõ) by Model')
    ax1.set_ylabel('‚Ñè‚Çõ Score')
    ax1.set_xticks(range(len(df)))
    ax1.set_xticklabels([m.replace('-', '\n').replace('claude-3', 'claude-3') for m in df['model']], 
                        rotation=45, ha='right', fontsize=9)
    ax1.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Collapse Threshold')
    ax1.legend()
    
    # Add value labels on bars
    for bar, val in zip(bars, df['hbar_s']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{val:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 2. ŒîŒº vs ŒîœÉ scatter plot
    ax2 = axes[0, 1]
    scatter = ax2.scatter(df['delta_mu'], df['delta_sigma'], 
                         c=df['hbar_s'], cmap='RdYlBu_r', s=100, alpha=0.7)
    ax2.set_title('Semantic Precision vs Flexibility')
    ax2.set_xlabel('ŒîŒº (Precision)')
    ax2.set_ylabel('ŒîœÉ (Flexibility)')
    
    # Add model labels
    for i, model in enumerate(df['model']):
        ax2.annotate(model.split('-')[0], (df.iloc[i]['delta_mu'], df.iloc[i]['delta_sigma']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.colorbar(scatter, ax=ax2, label='‚Ñè‚Çõ Score')
    
    # 3. Response length vs ‚Ñè‚Çõ
    ax3 = axes[0, 2]
    ax3.scatter(df['response_length'], df['hbar_s'], 
               c=colors, s=100, alpha=0.7)
    ax3.set_title('Response Length vs Semantic Uncertainty')
    ax3.set_xlabel('Response Length (chars)')
    ax3.set_ylabel('‚Ñè‚Çõ Score')
    
    # Add correlation coefficient
    correlation = np.corrcoef(df['response_length'], df['hbar_s'])[0, 1]
    ax3.text(0.05, 0.95, f'Correlation: {correlation:.3f}', 
             transform=ax3.transAxes, bbox=dict(boxstyle="round", facecolor='white', alpha=0.8))
    
    # 4. Model performance radar chart
    ax4 = axes[1, 0]
    ax4.remove()  # Remove and create polar subplot
    ax4 = fig.add_subplot(2, 3, 4, projection='polar')
    
    categories = ['Stability\n(1-‚Ñè‚Çõ)', 'Precision\n(1-ŒîŒº)', 'Coherence\n(1-ŒîœÉ)']
    N = len(categories)
    
    # Prepare data for radar chart (invert values so higher = better)
    radar_data = []
    for _, row in df.iterrows():
        values = [1 - row['hbar_s'], 1 - row['delta_mu'], 1 - row['delta_sigma']]
        radar_data.append(values)
    
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # Complete the circle
    
    # Plot each model
    colors_radar = plt.cm.tab10(np.linspace(0, 1, len(df)))
    for i, (_, row) in enumerate(df.iterrows()):
        values = radar_data[i]
        values += values[:1]  # Complete the circle
        ax4.plot(angles, values, 'o-', linewidth=2, label=row['model'].split('-')[0], 
                color=colors_radar[i], alpha=0.7)
        ax4.fill(angles, values, alpha=0.1, color=colors_radar[i])
    
    ax4.set_xticks(angles[:-1])
    ax4.set_xticklabels(categories, fontsize=10)
    ax4.set_ylim(0, 1)
    ax4.set_title('Model Performance Radar', pad=20)
    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=8)
    
    # 5. Metric distribution
    ax5 = axes[1, 1]
    metrics = ['hbar_s', 'delta_mu', 'delta_sigma']
    metric_data = [df[metric] for metric in metrics]
    
    box_plot = ax5.boxplot(metric_data, labels=['‚Ñè‚Çõ', 'ŒîŒº', 'ŒîœÉ'], patch_artist=True)
    colors_box = ['#ff9999', '#66b3ff', '#99ff99']
    for patch, color in zip(box_plot['boxes'], colors_box):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax5.set_title('Semantic Metric Distributions')
    ax5.set_ylabel('Score')
    ax5.grid(True, alpha=0.3)
    
    # 6. Model ranking table
    ax6 = axes[1, 2]
    ax6.axis('tight')
    ax6.axis('off')
    
    # Create ranking table
    ranking_data = []
    for i, (_, row) in enumerate(df.sort_values('hbar_s').iterrows(), 1):
        ranking_data.append([
            i,
            row['model'].replace('-', '\n'),
            f"{row['hbar_s']:.3f}",
            "üî¥" if row['collapse_risk'] else "üü¢"
        ])
    
    table = ax6.table(cellText=ranking_data,
                      colLabels=['Rank', 'Model', '‚Ñè‚Çõ Score', 'Status'],
                      cellLoc='center',
                      loc='center')
    
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2)
    ax6.set_title('Final Ranking', pad=20)
    
    plt.tight_layout()
    
    # Save visualization
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    viz_filename = f'universal_semantic_analysis_{timestamp}.png'
    plt.savefig(viz_filename, dpi=300, bbox_inches='tight')
    print(f"üìà Visualization saved to: {viz_filename}")
    
    # Show key insights
    print(f"\nüéØ KEY INSIGHTS:")
    print(f"=" * 50)
    
    # Stability ranking
    sorted_df = df.sort_values('hbar_s')
    print(f"üèÜ Most Stable Model: {sorted_df.iloc[0]['model']} (‚Ñè‚Çõ = {sorted_df.iloc[0]['hbar_s']:.3f})")
    print(f"‚ö†Ô∏è  Least Stable Model: {sorted_df.iloc[-1]['model']} (‚Ñè‚Çõ = {sorted_df.iloc[-1]['hbar_s']:.3f})")
    
    # Correlation insights
    length_corr = np.corrcoef(df['response_length'], df['hbar_s'])[0, 1]
    print(f"üìè Length-Uncertainty Correlation: {length_corr:.3f}")
    if abs(length_corr) < 0.3:
        print("   ‚Üí Response length has minimal impact on semantic uncertainty")
    elif length_corr > 0.3:
        print("   ‚Üí Longer responses tend to be more uncertain")
    else:
        print("   ‚Üí Shorter responses tend to be more uncertain")
    
    # Precision vs Flexibility insight
    precision_flexibility_corr = np.corrcoef(df['delta_mu'], df['delta_sigma'])[0, 1]
    print(f"‚öñÔ∏è  Precision-Flexibility Correlation: {precision_flexibility_corr:.3f}")
    
    # Category performance analysis
    print(f"\nüìä UNIVERSAL PROMPT EFFECTIVENESS:")
    print(f"‚Ä¢ All models showed semantic collapse (‚Ñè‚Çõ < 1.0)")
    print(f"‚Ä¢ Collapse rate: {(df['collapse_risk'].sum() / len(df)) * 100:.0f}%")
    print(f"‚Ä¢ ‚Ñè‚Çõ range: {df['hbar_s'].min():.3f} - {df['hbar_s'].max():.3f}")
    print(f"‚Ä¢ Standard deviation: {df['hbar_s'].std():.3f}")
    
    # Model family analysis
    print(f"\nüè¢ MODEL FAMILY ANALYSIS:")
    gpt_models = df[df['model'].str.contains('gpt')]
    claude_models = df[df['model'].str.contains('claude')]
    mistral_models = df[df['model'].str.contains('mistral')]
    
    if len(gpt_models) > 0:
        print(f"‚Ä¢ OpenAI GPT avg ‚Ñè‚Çõ: {gpt_models['hbar_s'].mean():.3f}")
    if len(claude_models) > 0:
        print(f"‚Ä¢ Anthropic Claude avg ‚Ñè‚Çõ: {claude_models['hbar_s'].mean():.3f}")
    if len(mistral_models) > 0:
        print(f"‚Ä¢ Mistral avg ‚Ñè‚Çõ: {mistral_models['hbar_s'].mean():.3f}")
    
    print(f"\n‚úÖ Analysis complete! The universal prompt successfully differentiated model performance.")
    print(f"üéØ Single test prompt effectively measured semantic robustness across 6 cognitive categories.")

if __name__ == "__main__":
    try:
        analyze_responses()
    except ImportError as e:
        if 'matplotlib' in str(e) or 'seaborn' in str(e):
            print("üì¶ Installing visualization dependencies...")
            import subprocess
            subprocess.run(['pip', 'install', 'matplotlib', 'seaborn'], check=True)
            analyze_responses()
        else:
            raise 