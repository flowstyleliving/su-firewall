[
  {
    "model": "gpt4",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.2310451622579534,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.057282196186948006,
    "hbar_s": 0.27358892849834227,
    "delta_mu": 0.736675069649855,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1889530073385686,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.008,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. Paris is the capital of France."
  },
  {
    "model": "claude3",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27920360098000374,
    "delta_mu": 0.7626753662257322,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1839330040384084,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.011,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. Paris is the capital of France."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2700504436601707,
    "delta_mu": 0.7106747730739778,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1862150062806904,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.013,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. Paris is the capital of France."
  },
  {
    "model": "gemini",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2675267570031266,
    "delta_mu": 0.693341242023393,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1402879976667464,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.016,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, Paris is the capital of France."
  },
  {
    "model": "openai_o3",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28532252557773324,
    "delta_mu": 0.8060091938521943,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 3.1145619868766516,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.005,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, Paris is the capital of France."
  },
  {
    "model": "grok3",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.259796525923772,
    "delta_mu": 0.6500074143969309,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1134270248003304,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.019,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! Paris is the capital of France."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2773887747274859,
    "delta_mu": 0.736675069649855,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1122330110520124,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.022,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: I believe it's Paris."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is the capital of France?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22029291863913625,
    "raw_delta_mu": 0.9319102715368184,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.25492820812695394,
    "delta_mu": 0.6066735867704688,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.087204007897526,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.035,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: I believe it's Paris."
  },
  {
    "model": "gpt4",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27910662153084015,
    "delta_mu": 0.7606401656638094,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.7038850132375956,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.012,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. World War II ended in 1945."
  },
  {
    "model": "claude3",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2852521217874212,
    "delta_mu": 0.7874862891578263,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.7011819949839264,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.0165,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. World War II ended in 1945."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27616861425566624,
    "delta_mu": 0.7337940421697927,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.6972320231143385,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.0195,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. World War II ended in 1945."
  },
  {
    "model": "gemini",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27398394497421275,
    "delta_mu": 0.7158966265071148,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.6921319842804223,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.024,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, It ended in 1945, I think."
  },
  {
    "model": "openai_o3",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2906475681417413,
    "delta_mu": 0.832229828314521,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 3.689342993311584,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.0075,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, World War II ended in 1945."
  },
  {
    "model": "grok3",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2664496120140906,
    "delta_mu": 0.6711530873504201,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.685924020828679,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.0285,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! It ended in 1945, I think."
  },
  {
    "model": "gemini_flash",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28489835972466193,
    "delta_mu": 0.7606401656638094,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.687633987283334,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.033,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: It ended in 1945, I think."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What year did World War II end?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818239512691207,
    "raw_delta_mu": 0.9998556236132888,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2634215427923987,
    "delta_mu": 0.6264095481937254,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.6899610131513327,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.15,
    "uncertainty_amplification": 1.0525,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: It ended in 1945, I think."
  },
  {
    "model": "gpt4",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28168430021005514,
    "delta_mu": 0.7784426104565394,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.058047004742548,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0096,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "claude3",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28763408851964384,
    "delta_mu": 0.8059170555314761,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 3.056235989788547,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0132,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.278312976297232,
    "delta_mu": 0.7509681653816027,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.9810650157742202,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0156,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "gemini",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2758723953581761,
    "delta_mu": 0.7326518686649783,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.936081000370905,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0192,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "openai_o3",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.29359137459598805,
    "delta_mu": 0.8517077973230374,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.9358580068219453,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.006,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "grok3",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2680557875045862,
    "delta_mu": 0.686861126873417,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.9378449835348874,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0228,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! William Shakespeare wrote Romeo and Juliet."
  },
  {
    "model": "gemini_flash",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28637159839104653,
    "delta_mu": 0.7784426104565394,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.9406200046651065,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.0264,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: Shakespeare wrote it."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Who wrote Romeo and Juliet?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22817580327084344,
    "raw_delta_mu": 0.9997978557109419,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.26382781232993924,
    "delta_mu": 0.6410703850818559,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.9296180000528693,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.12,
    "uncertainty_amplification": 1.042,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: Shakespeare wrote it."
  },
  {
    "model": "gpt4",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2764743362627378,
    "delta_mu": 0.7428329629432527,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.730696985963732,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0144,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28280850396759877,
    "delta_mu": 0.7690505969294852,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.7344660193193704,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0198,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2739608213306663,
    "delta_mu": 0.7166153289570202,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.701360994251445,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0234,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2720270361963947,
    "delta_mu": 0.699136906299532,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.6935609930660576,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0288,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28765290296057555,
    "delta_mu": 0.812746653573206,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.6750380056910217,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.009,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2647715304951913,
    "delta_mu": 0.6554408496558112,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.665375010110438,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0342,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2833425867298327,
    "delta_mu": 0.7428329629432527,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.6638930139597505,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.0396,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is the largest planet in our solar system?",
    "category": "basic_facts",
    "tier": 1,
    "raw_hbar_s": 0.22818857276429064,
    "raw_delta_mu": 0.9999097630142048,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.26291684084802913,
    "delta_mu": 0.6117447930120905,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.6604990125633776,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.18,
    "uncertainty_amplification": 1.063,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27770116364536884,
    "delta_mu": 0.7650467481666154,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.2360829936806113,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.004,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. 4"
  },
  {
    "model": "claude3",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28298142870956594,
    "delta_mu": 0.7920483981019077,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1729930196888745,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.0055,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. 4"
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27343570851549925,
    "delta_mu": 0.7380450982313231,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.124479000689462,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.0065,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. 4"
  },
  {
    "model": "gemini",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2704830466152625,
    "delta_mu": 0.7200439982744616,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.117473010206595,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.008,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, The answer is 4."
  },
  {
    "model": "openai_o3",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2900416582671715,
    "delta_mu": 0.8370511479940617,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 3.111127996817231,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.0025,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, 4"
  },
  {
    "model": "grok3",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.26228380708199245,
    "delta_mu": 0.6750412483823078,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.1105889938771725,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.0095,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! The answer is 4."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2796373271369202,
    "delta_mu": 0.7650467481666154,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.130552009679377,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.011,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: 4"
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is 2 + 2?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22038617877724115,
    "raw_delta_mu": 0.9326994796301317,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2553982762720751,
    "delta_mu": 0.6300384984901539,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 3.133656980935484,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.05,
    "uncertainty_amplification": 1.0175,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: The answer is 4."
  },
  {
    "model": "gpt4",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28653011550487323,
    "delta_mu": 0.8131699043955651,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.7432039787527174,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0048,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2920649542092966,
    "delta_mu": 0.8418700186683499,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.7237760077696294,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0066,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28226853278230063,
    "delta_mu": 0.7844697901227805,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.7276049950160086,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0078,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2793029488773961,
    "delta_mu": 0.7653363806075908,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.7315889892634004,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0096,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2991738242298187,
    "delta_mu": 0.8897035424563242,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.7342080138623714,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0030000000000001,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27091606977556276,
    "delta_mu": 0.7175028568196163,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.7405650180298835,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0114,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2889254707698423,
    "delta_mu": 0.8131699043955651,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.7271219878457487,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.0132,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is 10 - 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22804046540251227,
    "raw_delta_mu": 0.9986121876403846,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.26421407403748154,
    "delta_mu": 0.6696693330316418,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.7310520235914737,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.06,
    "uncertainty_amplification": 1.021,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2850487805433,
    "delta_mu": 0.8022267117753847,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.289479976752773,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0064,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.29072709321093726,
    "delta_mu": 0.8305405957203984,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.290415985044092,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0088,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.281086093761449,
    "delta_mu": 0.7739128278303711,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.286967996042222,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0104,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27829652445189457,
    "delta_mu": 0.7550369052003622,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.2798719874117523,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0128,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.29745021250626485,
    "delta_mu": 0.8777304022954211,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.2759510029572994,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.004,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27009798063243945,
    "delta_mu": 0.7078470986253395,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.274205005960539,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0152,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2882210245239091,
    "delta_mu": 0.8022267117753847,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.2735540114808828,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.0176,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is 5 \u00d7 4?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.2281742264681042,
    "raw_delta_mu": 0.9997840376064118,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2642294562917053,
    "delta_mu": 0.6606572920503169,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.2761549917049706,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.08,
    "uncertainty_amplification": 1.028,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2833011874096356,
    "delta_mu": 0.7899065687565889,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.231823018519208,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.008,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28911517772606227,
    "delta_mu": 0.8177856241244685,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.2407459910027683,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.011,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.27963708827453093,
    "delta_mu": 0.7620275133887092,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.255925995996222,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.013,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.277023812106834,
    "delta_mu": 0.7434414764767896,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.312325988896191,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.016,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.29545132083580544,
    "delta_mu": 0.8642507164042679,
    "delta_sigma": 0.1,
    "confidence_flag": "\u2705",
    "latency_ms": 2.356293989578262,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.005,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.26901916200731335,
    "delta_mu": 0.6969763841969903,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.3776900197844952,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.019,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.28723592612365834,
    "delta_mu": 0.7899065687565889,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.390502020716667,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.022,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is 15 \u00f7 3?",
    "category": "basic_math",
    "tier": 1,
    "raw_hbar_s": 0.22811319804112506,
    "raw_delta_mu": 0.9992492963397709,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2639780215631583,
    "delta_mu": 0.6505112919171908,
    "delta_sigma": 0.1,
    "confidence_flag": "\u26a0\ufe0f",
    "latency_ms": 2.402711019385606,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.1,
    "uncertainty_amplification": 1.035,
    "collapse_risk": "False",
    "risk_level": "MEDIUM",
    "recommendations": [],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2173113329746256,
    "delta_mu": 0.4035487905771978,
    "delta_sigma": 0.10414944761263273,
    "confidence_flag": "\u274c",
    "latency_ms": 2.410913002677262,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.06,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This is a classical paradox with no clear answer."
  },
  {
    "model": "claude3",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22126249398579145,
    "delta_mu": 0.4177916890681577,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.412149013252929,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.0825,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a classical paradox with no clear answer."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22779436261557443,
    "delta_mu": 0.3893058920862379,
    "delta_sigma": 0.11065878808842228,
    "confidence_flag": "\u274c",
    "latency_ms": 2.4172620032913983,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.0975,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This paradox reveals the logical tensions in omnipotence concepts. It's fundamentally unanswerable within classical logic."
  },
  {
    "model": "gemini",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21995425694868928,
    "delta_mu": 0.379810626425598,
    "delta_sigma": 0.10154571142231691,
    "confidence_flag": "\u274c",
    "latency_ms": 2.420881006401032,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.12,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a classical paradox with no clear answer."
  },
  {
    "model": "openai_o3",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21800584707005677,
    "delta_mu": 0.44152985321975763,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.4245399981737137,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.0375,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This is a classical paradox with no clear answer."
  },
  {
    "model": "grok3",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2333620059581571,
    "delta_mu": 0.3560724622739981,
    "delta_sigma": 0.11716812856421183,
    "confidence_flag": "\u274c",
    "latency_ms": 2.4273089948110282,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.1425,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This paradox reveals the logical tensions in omnipotence concepts. It's fundamentally unanswerable within classical logic."
  },
  {
    "model": "gemini_flash",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2340313028829984,
    "delta_mu": 0.4035487905771978,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.429750020382926,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.165,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a classical paradox with no clear answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Can an omnipotent being create a stone so heavy they cannot lift it?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22814200027393497,
    "raw_delta_mu": 0.9995016484884155,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24210955182686952,
    "delta_mu": 0.3323342981223982,
    "delta_sigma": 0.11065878808842228,
    "confidence_flag": "\u274c",
    "latency_ms": 2.433913992717862,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.75,
    "uncertainty_amplification": 1.2625,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This paradox reveals the logical tensions in omnipotence concepts. It's fundamentally unanswerable within classical logic."
  },
  {
    "model": "gpt4",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21886657480551133,
    "delta_mu": 0.421317220062882,
    "delta_sigma": 0.10164986086992955,
    "confidence_flag": "\u274c",
    "latency_ms": 2.836717991158366,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.0575999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22539195723735445,
    "delta_mu": 0.43618723959451317,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8488280076999217,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.0792,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22912814204338638,
    "delta_mu": 0.40644720053125083,
    "delta_sigma": 0.10800297717430013,
    "confidence_flag": "\u274c",
    "latency_ms": 2.855869010090828,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.0936,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22207153595616586,
    "delta_mu": 0.3965338541768302,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8638509975280613,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.1152,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22243154159872847,
    "delta_mu": 0.46097060548056507,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.870813012123108,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.036,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23439022408780555,
    "delta_mu": 0.3717504882907782,
    "delta_sigma": 0.11435609347867073,
    "confidence_flag": "\u274c",
    "latency_ms": 2.876206999644637,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.1368,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23777333752290727,
    "delta_mu": 0.421317220062882,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8804310131818056,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.1583999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "If Pinocchio says 'My nose will grow now', what happens?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22812232618910588,
    "raw_delta_mu": 0.9993292695988663,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24236301954812048,
    "delta_mu": 0.3469671224047263,
    "delta_sigma": 0.10800297717430013,
    "confidence_flag": "\u274c",
    "latency_ms": 2.87938499241136,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.72,
    "uncertainty_amplification": 1.252,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21550423375726965,
    "delta_mu": 0.38581451430740077,
    "delta_sigma": 0.10664903435533592,
    "confidence_flag": "\u274c",
    "latency_ms": 3.539485012879595,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.0624,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2170056250343912,
    "delta_mu": 0.399431497165309,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.5294950066599995,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.0857999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2261907025286276,
    "delta_mu": 0.37219753144949247,
    "delta_sigma": 0.11331459900254441,
    "confidence_flag": "\u274c",
    "latency_ms": 3.5245730250608176,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.1014,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21856513766456087,
    "delta_mu": 0.36311954287755366,
    "delta_sigma": 0.10398280849645251,
    "confidence_flag": "\u274c",
    "latency_ms": 3.5203279985580593,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.1248,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21346999496517344,
    "delta_mu": 0.4221264685951562,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.500866994727403,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.0390000000000001,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2320506115063024,
    "delta_mu": 0.34042457144770655,
    "delta_sigma": 0.1199801636497529,
    "confidence_flag": "\u274c",
    "latency_ms": 3.4947960230056196,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.1481999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.230127565898161,
    "delta_mu": 0.38581451430740077,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.489333001198247,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.1716,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Is the statement 'I am lying' true or false?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22817359194644182,
    "raw_delta_mu": 0.9997784770857754,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24154618066063122,
    "delta_mu": 0.3177296000178595,
    "delta_sigma": 0.11331459900254441,
    "confidence_flag": "\u274c",
    "latency_ms": 3.483457025140524,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.78,
    "uncertainty_amplification": 1.273,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2224977068811757,
    "delta_mu": 0.44528727482340763,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 6.651211995631456,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.0544,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23077017166334007,
    "delta_mu": 0.4610032962877632,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 6.612981989746913,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.0748,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23056059834004033,
    "delta_mu": 0.42957125335905205,
    "delta_sigma": 0.10446189595547063,
    "confidence_flag": "\u274c",
    "latency_ms": 6.6080270044039935,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.0884,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22699113166889645,
    "delta_mu": 0.4190939057161484,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 6.637134007178247,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.1088,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22822998006113937,
    "delta_mu": 0.48719666539502254,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 6.646066001849249,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.034,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2353980843479474,
    "delta_mu": 0.39290053660888913,
    "delta_sigma": 0.11060671336461597,
    "confidence_flag": "\u274c",
    "latency_ms": 6.656719982856885,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.1292,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24258665006695712,
    "delta_mu": 0.44528727482340763,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 6.688407011097297,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.1496,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What happens when an unstoppable force meets an immovable object?",
    "category": "logical_paradox",
    "tier": 2,
    "raw_hbar_s": 0.22816999001660268,
    "raw_delta_mu": 0.9997469124908119,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24230328156699882,
    "delta_mu": 0.3667071675016298,
    "delta_sigma": 0.10446189595547063,
    "confidence_flag": "\u274c",
    "latency_ms": 6.693709990940988,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.68,
    "uncertainty_amplification": 1.238,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22638695182840696,
    "delta_mu": 0.4630962927590163,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.897307014791295,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.052,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This involves synesthetic metaphor - perhaps silence could be imagined as transparent or deep blue, representing the absence of auditory stimulation."
  },
  {
    "model": "claude3",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23461712074548616,
    "delta_mu": 0.47944086779756984,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9061200038995594,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.0715,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a category error - silence doesn't have color."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23128610389672113,
    "delta_mu": 0.4467517177204628,
    "delta_sigma": 0.10180608504134848,
    "confidence_flag": "\u274c",
    "latency_ms": 2.903782995417714,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.0845,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This involves synesthetic metaphor - perhaps silence could be imagined as transparent or deep blue, representing the absence of auditory stimulation."
  },
  {
    "model": "gemini",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23048372072774626,
    "delta_mu": 0.43585533436142715,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.899690007325262,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.104,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This involves synesthetic metaphor - perhaps silence could be imagined as transparent or deep blue, representing the absence of auditory stimulation."
  },
  {
    "model": "openai_o3",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2324115590018627,
    "delta_mu": 0.506681826195159,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8938899922650307,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.0325,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This is a category error - silence doesn't have color."
  },
  {
    "model": "grok3",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23579172414913924,
    "delta_mu": 0.40861437596383793,
    "delta_sigma": 0.10779467827907487,
    "confidence_flag": "\u274c",
    "latency_ms": 2.888159011490643,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.1235,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This involves synesthetic metaphor - perhaps silence could be imagined as transparent or deep blue, representing the absence of auditory stimulation."
  },
  {
    "model": "gemini_flash",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2459698535550087,
    "delta_mu": 0.4630962927590163,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8815430123358965,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.143,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a category error - silence doesn't have color."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Describe the color of silence.",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22816100743291004,
    "raw_delta_mu": 0.9996681980766676,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24187087659256015,
    "delta_mu": 0.3813734175662487,
    "delta_sigma": 0.10180608504134848,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8762709989678115,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.65,
    "uncertainty_amplification": 1.2275,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This involves synesthetic metaphor - perhaps silence could be imagined as transparent or deep blue, representing the absence of auditory stimulation."
  },
  {
    "model": "gpt4",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23014473901916196,
    "delta_mu": 0.48078892948437185,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.165007015923038,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.0495999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23832065094456384,
    "delta_mu": 0.4977579505249968,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.195581986801699,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.0682,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2327231872386799,
    "delta_mu": 0.463819908443747,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.202816005796194,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.0806,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2338242122752223,
    "delta_mu": 0.4525072277499971,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.207596979336813,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.0992,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23646556510521236,
    "delta_mu": 0.5260396522593717,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.207312023732811,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.0310000000000001,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23589636849576762,
    "delta_mu": 0.42422552601562225,
    "delta_sigma": 0.1049826431935338,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2164079893846065,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.1178,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2491772879395729,
    "delta_mu": 0.48078892948437185,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2283299951814115,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.1364,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What does the number 7 taste like?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281250824359355,
    "raw_delta_mu": 0.9993534181757885,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24216276401645412,
    "delta_mu": 0.3959438242812474,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2325249921996146,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.62,
    "uncertainty_amplification": 1.217,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21982459431091514,
    "delta_mu": 0.43333595421507703,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2482170208822936,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.056,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2281182013606958,
    "delta_mu": 0.4486301643638444,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.27036899398081,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.077,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22991279832725112,
    "delta_mu": 0.41804174406630956,
    "delta_sigma": 0.10623243656488537,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2270470110233873,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.091,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2245704865987364,
    "delta_mu": 0.4078456039671313,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.184611996402964,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.112,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2253640939158266,
    "delta_mu": 0.47412051461179017,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.1802589946892112,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.0350000000000001,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23496534015762097,
    "delta_mu": 0.3823552537191856,
    "delta_sigma": 0.11248140342164335,
    "confidence_flag": "\u274c",
    "latency_ms": 3.1806949991732836,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.133,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2402249827981023,
    "delta_mu": 0.43333595421507703,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.1992669973988086,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.154,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "How heavy is the concept of justice?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.22815568720383142,
    "raw_delta_mu": 0.999621578350812,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2424096151291884,
    "delta_mu": 0.35686490347123984,
    "delta_sigma": 0.10623243656488537,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2018389902077615,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.7,
    "uncertainty_amplification": 1.2449999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2238249293081259,
    "delta_mu": 0.45130005670787227,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2180710113607347,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.0535999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23208523946856416,
    "delta_mu": 0.4672282940034443,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2175689993891865,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.0737,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23085034731968357,
    "delta_mu": 0.4353718194123003,
    "delta_sigma": 0.10357662565076324,
    "confidence_flag": "\u274c",
    "latency_ms": 3.21515099494718,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.0871,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22818878807528553,
    "delta_mu": 0.42475299454858567,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.210478986147791,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.1072,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2296546199010783,
    "delta_mu": 0.4937753561627309,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.1917519809212536,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.0335,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23557871087281404,
    "delta_mu": 0.3982059323892991,
    "delta_sigma": 0.10966936833610226,
    "confidence_flag": "\u274c",
    "latency_ms": 3.203722008038312,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.1273,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24375163618844314,
    "delta_mu": 0.45130005670787227,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2074350165203214,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.1474,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is the temperature of happiness?",
    "category": "impossible_description",
    "tier": 2,
    "raw_hbar_s": 0.2281862431763007,
    "raw_delta_mu": 0.9998893468657855,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.24221120184581468,
    "delta_mu": 0.3716588702300125,
    "delta_sigma": 0.10357662565076324,
    "confidence_flag": "\u274c",
    "latency_ms": 3.2065199920907617,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.67,
    "uncertainty_amplification": 1.2345,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.1995331938873591,
    "delta_mu": 0.2846366410796224,
    "delta_sigma": 0.12081335923065396,
    "confidence_flag": "\u274c",
    "latency_ms": 3.010106011060998,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.076,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This creates a self-referential paradox: if true, then false; if false, then true. It demonstrates the limits of classical truth values."
  },
  {
    "model": "claude3",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2017837155048031,
    "delta_mu": 0.29468264017655027,
    "delta_sigma": 0.11326252427873809,
    "confidence_flag": "\u274c",
    "latency_ms": 2.981262019602582,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.1045,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a logical paradox without a clear truth value."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21092977645847855,
    "delta_mu": 0.2745906419826945,
    "delta_sigma": 0.12836419418256984,
    "confidence_flag": "\u274c",
    "latency_ms": 2.993086993228644,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.1235,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a logical paradox without a clear truth value."
  },
  {
    "model": "gemini",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20464125970487443,
    "delta_mu": 0.2678933092514093,
    "delta_sigma": 0.1177930252498876,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9948869778309017,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.1520000000000001,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a logical paradox without a clear truth value."
  },
  {
    "model": "openai_o3",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.1900609748583094,
    "delta_mu": 0.31142597200476335,
    "delta_sigma": 0.1057116893268222,
    "confidence_flag": "\u274c",
    "latency_ms": 2.977133000968024,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.0475,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This creates a self-referential paradox: if true, then false; if false, then true. It demonstrates the limits of classical truth values."
  },
  {
    "model": "grok3",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21810534282562155,
    "delta_mu": 0.2511499774231962,
    "delta_sigma": 0.13591502913448572,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9883920215070248,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.1804999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a logical paradox without a clear truth value."
  },
  {
    "model": "gemini_flash",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20397253986012517,
    "delta_mu": 0.2846366410796224,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.0033430084586143,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.209,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a logical paradox without a clear truth value."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "If this statement is false, what is its truth value?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22815344150101777,
    "raw_delta_mu": 0.9996019001918256,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2311394774735826,
    "delta_mu": 0.23440664559498314,
    "delta_sigma": 0.12836419418256984,
    "confidence_flag": "\u274c",
    "latency_ms": 3.0097540002316236,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.95,
    "uncertainty_amplification": 1.3325,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a logical paradox without a clear truth value."
  },
  {
    "model": "gpt4",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20735678168263602,
    "delta_mu": 0.3263760201303448,
    "delta_sigma": 0.11498099016434653,
    "confidence_flag": "\u274c",
    "latency_ms": 2.6461679954081774,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.0704,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20932307041283188,
    "delta_mu": 0.337895173782004,
    "delta_sigma": 0.10779467827907487,
    "confidence_flag": "\u274c",
    "latency_ms": 2.672290022019297,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.0968,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21856226333450524,
    "delta_mu": 0.31485686647868555,
    "delta_sigma": 0.12216730204961818,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7120240265503526,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.1144,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21169933523203682,
    "delta_mu": 0.30717743071091275,
    "delta_sigma": 0.11210646541023786,
    "confidence_flag": "\u274c",
    "latency_ms": 2.741153002716601,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.1408,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19788308840394017,
    "delta_mu": 0.35709376320143604,
    "delta_sigma": 0.1006083663938032,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7221979980822653,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.044,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22527596162744082,
    "delta_mu": 0.28797884129148066,
    "delta_sigma": 0.12935361393488984,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7080080180894583,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.1672,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2156343436654465,
    "delta_mu": 0.3263760201303448,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7114140102639794,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.1936,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Can you think of something you've never thought of?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22819048620430213,
    "raw_delta_mu": 0.9999265322620857,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23701941057448206,
    "delta_mu": 0.2687802518720486,
    "delta_sigma": 0.12216730204961818,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7150610112585127,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.88,
    "uncertainty_amplification": 1.308,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2031100501531927,
    "delta_mu": 0.30251193872879234,
    "delta_sigma": 0.11831377248795077,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8948410181328654,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.0735999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2052450182885485,
    "delta_mu": 0.3131888306839262,
    "delta_sigma": 0.11091916170745385,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8697829984594136,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.1012,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21444391783086209,
    "delta_mu": 0.2918350467736585,
    "delta_sigma": 0.1257083832684477,
    "confidence_flag": "\u274c",
    "latency_ms": 2.870405005523935,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.1196,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2079054564056945,
    "delta_mu": 0.2847171188035693,
    "delta_sigma": 0.115355928175752,
    "confidence_flag": "\u274c",
    "latency_ms": 2.873375022318214,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.1472,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19362286100144238,
    "delta_mu": 0.3309836506091493,
    "delta_sigma": 0.10352455092695692,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8695299988612533,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.046,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22143704560233918,
    "delta_mu": 0.26692229887834623,
    "delta_sigma": 0.13310299404894463,
    "confidence_flag": "\u274c",
    "latency_ms": 2.868801006115973,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.1747999999999998,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20913187299053818,
    "delta_mu": 0.30251193872879234,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8640230011660606,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.2024,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Is the question 'Why is there something rather than nothing?' meaningful?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22816566187271942,
    "raw_delta_mu": 0.9997089845630944,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23395072330418143,
    "delta_mu": 0.2491274789531231,
    "delta_sigma": 0.1257083832684477,
    "confidence_flag": "\u274c",
    "latency_ms": 2.8651970205828547,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.92,
    "uncertainty_amplification": 1.322,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20530832275790786,
    "delta_mu": 0.31444778938433443,
    "delta_sigma": 0.11664738132614866,
    "confidence_flag": "\u274c",
    "latency_ms": 2.5582369999028742,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.072,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20736094986240722,
    "delta_mu": 0.3255459466567227,
    "delta_sigma": 0.10935691999326436,
    "confidence_flag": "\u274c",
    "latency_ms": 2.55333399400115,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.099,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2165842519900173,
    "delta_mu": 0.30334963211194615,
    "delta_sigma": 0.12393784265903293,
    "confidence_flag": "\u274c",
    "latency_ms": 2.548771008150652,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.117,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2098822119970615,
    "delta_mu": 0.2959508605970207,
    "delta_sigma": 0.11373119679299493,
    "confidence_flag": "\u274c",
    "latency_ms": 2.543540991609916,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.1440000000000001,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.1958231796643456,
    "delta_mu": 0.3440428754440365,
    "delta_sigma": 0.10206645866038005,
    "confidence_flag": "\u274c",
    "latency_ms": 2.544879011111334,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.045,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22344266104713145,
    "delta_mu": 0.27745393180970684,
    "delta_sigma": 0.13122830399191723,
    "confidence_flag": "\u274c",
    "latency_ms": 2.539537992561236,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.171,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2124374555288107,
    "delta_mu": 0.31444778938433443,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.540238026995212,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.198,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What is the difference between existing and not existing?",
    "category": "existential_paradox",
    "tier": 3,
    "raw_hbar_s": 0.22817992638153942,
    "raw_delta_mu": 0.9998339885034482,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23558169873448745,
    "delta_mu": 0.25895700302239305,
    "delta_sigma": 0.12393784265903293,
    "confidence_flag": "\u274c",
    "latency_ms": 2.5427269865758717,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.9,
    "uncertainty_amplification": 1.315,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2018864283087897,
    "delta_mu": 0.2963461263791747,
    "delta_sigma": 0.11914696806885186,
    "confidence_flag": "\u274c",
    "latency_ms": 3.182181011652574,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.0744,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This is a meta-question about questions."
  },
  {
    "model": "claude3",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20406026387635304,
    "delta_mu": 0.30680540142785145,
    "delta_sigma": 0.11170028256454861,
    "confidence_flag": "\u274c",
    "latency_ms": 3.184242988936603,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.1023,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a meta-question about questions."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21324061527308594,
    "delta_mu": 0.2858868513304979,
    "delta_sigma": 0.12659365357315508,
    "confidence_flag": "\u274c",
    "latency_ms": 3.0474549857899547,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.1209,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This meta-question examines the recursive nature of questioning itself - a question about the nature of questions."
  },
  {
    "model": "gemini",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20678707514384626,
    "delta_mu": 0.2789140012980468,
    "delta_sigma": 0.11616829386713055,
    "confidence_flag": "\u274c",
    "latency_ms": 3.0260019993875176,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.1488,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a meta-question about questions."
  },
  {
    "model": "openai_o3",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19240501856101003,
    "delta_mu": 0.32423752650897936,
    "delta_sigma": 0.10425359706024537,
    "confidence_flag": "\u274c",
    "latency_ms": 3.014424000866711,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.0465,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This is a meta-question about questions."
  },
  {
    "model": "grok3",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2202948318217131,
    "delta_mu": 0.2614818762169188,
    "delta_sigma": 0.13404033907745835,
    "confidence_flag": "\u274c",
    "latency_ms": 3.0067339830566198,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.1766999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This meta-question examines the recursive nature of questioning itself - a question about the nature of questions."
  },
  {
    "model": "gemini_flash",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20736835677250082,
    "delta_mu": 0.2963461263791747,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.000275988597423,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.2046,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a meta-question about questions."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Is the question 'What is the question?' a question?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22808196120922963,
    "raw_delta_mu": 0.9989756493483051,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2329833489783712,
    "delta_mu": 0.2440497511357909,
    "delta_sigma": 0.12659365357315508,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9944409907329828,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.93,
    "uncertainty_amplification": 1.3255,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This meta-question examines the recursive nature of questioning itself - a question about the nature of questions."
  },
  {
    "model": "gpt4",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19827229971115867,
    "delta_mu": 0.27871104546289194,
    "delta_sigma": 0.12164655481155504,
    "confidence_flag": "\u274c",
    "latency_ms": 2.6858330238610506,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.0768,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20055917683674837,
    "delta_mu": 0.288547905890994,
    "delta_sigma": 0.11404364513583283,
    "confidence_flag": "\u274c",
    "latency_ms": 2.6954619970638305,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.1056,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2096834904494191,
    "delta_mu": 0.2688741850347898,
    "delta_sigma": 0.1292494644872772,
    "confidence_flag": "\u274c",
    "latency_ms": 2.701577002881095,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.1248,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20347922876233937,
    "delta_mu": 0.2623162780827218,
    "delta_sigma": 0.11860539094126615,
    "confidence_flag": "\u274c",
    "latency_ms": 2.705599006731063,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.1536,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.18880970669685557,
    "delta_mu": 0.3049426732711641,
    "delta_sigma": 0.10644073546011064,
    "confidence_flag": "\u274c",
    "latency_ms": 2.71843298105523,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.048,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21691463187151466,
    "delta_mu": 0.2459215107025517,
    "delta_sigma": 0.13685237416299942,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7168179803993553,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.1824,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20220549445604832,
    "delta_mu": 0.27871104546289194,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.719387994147837,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.2112,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Can the concept of concepts conceptualize itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.2281624612664592,
    "raw_delta_mu": 0.9996809378152506,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2301110539984297,
    "delta_mu": 0.22952674332238154,
    "delta_sigma": 0.1292494644872772,
    "confidence_flag": "\u274c",
    "latency_ms": 2.7182920021004975,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.96,
    "uncertainty_amplification": 1.3359999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20073631347080934,
    "delta_mu": 0.2905118999882969,
    "delta_sigma": 0.1199801636497529,
    "confidence_flag": "\u274c",
    "latency_ms": 3.8756690046284348,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.0752,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20294912320312655,
    "delta_mu": 0.3007652611643545,
    "delta_sigma": 0.11248140342164334,
    "confidence_flag": "\u274c",
    "latency_ms": 3.8782500196248293,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.1034,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21211378096497335,
    "delta_mu": 0.2802585388122394,
    "delta_sigma": 0.12747892387786244,
    "confidence_flag": "\u274c",
    "latency_ms": 3.914329019607976,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.1222,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.205742209683146,
    "delta_mu": 0.2734229646948677,
    "delta_sigma": 0.11698065955850907,
    "confidence_flag": "\u274c",
    "latency_ms": 3.9412689802702516,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.1504,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19125791101611803,
    "delta_mu": 0.31785419645778373,
    "delta_sigma": 0.10498264319353377,
    "confidence_flag": "\u274c",
    "latency_ms": 3.9584250189363956,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.0470000000000002,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.2192302883936347,
    "delta_mu": 0.25633402940143846,
    "delta_sigma": 0.13497768410597202,
    "confidence_flag": "\u274c",
    "latency_ms": 3.9737689949106425,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.1785999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20569193553982906,
    "delta_mu": 0.2905118999882969,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 3.988501994172111,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.2067999999999999,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "What category does the act of categorization belong to?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22812502798837786,
    "raw_delta_mu": 0.9993529411362121,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.23209495254651064,
    "delta_mu": 0.2392450941080092,
    "delta_sigma": 0.12747892387786244,
    "confidence_flag": "\u274c",
    "latency_ms": 4.003875015769154,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.94,
    "uncertainty_amplification": 1.329,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  },
  {
    "model": "gpt4",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.19696571663074472,
    "delta_mu": 0.27277327811476165,
    "delta_sigma": 0.1224797503924561,
    "confidence_flag": "\u274c",
    "latency_ms": 2.97408300684765,
    "latency_compliant": true,
    "model_semantic_capacity": 0.92,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.0776,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Let me think about this carefully. This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "claude3",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.1992876927518893,
    "delta_mu": 0.28240057028351795,
    "delta_sigma": 0.11482476599292758,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9451499867718667,
    "latency_compliant": true,
    "model_semantic_capacity": 0.89,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.1067,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "I appreciate this question. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_2.5_pro",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20838763648682365,
    "delta_mu": 0.2631459859460053,
    "delta_sigma": 0.1301347347919846,
    "confidence_flag": "\u274c",
    "latency_ms": 2.929587004473433,
    "latency_compliant": true,
    "model_semantic_capacity": 0.87,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.1261,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "This is an interesting inquiry. This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20226841742860444,
    "delta_mu": 0.2567277911668345,
    "delta_sigma": 0.11941775663264469,
    "confidence_flag": "\u274c",
    "latency_ms": 2.920919010648504,
    "latency_compliant": true,
    "model_semantic_capacity": 0.84,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.1552,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Based on my knowledge, This is a complex question that challenges conventional thinking."
  },
  {
    "model": "openai_o3",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.1875156546896597,
    "delta_mu": 0.2984460572314451,
    "delta_sigma": 0.10716978159339907,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9194919916335493,
    "latency_compliant": true,
    "model_semantic_capacity": 0.95,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.0485,
    "collapse_risk": "True",
    "risk_level": "HIGH",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "After careful reasoning, This requires careful philosophical consideration of the concepts involved."
  },
  {
    "model": "grok3",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.21567123097389376,
    "delta_mu": 0.24068230421890735,
    "delta_sigma": 0.1377897191915131,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9146989982109517,
    "latency_compliant": true,
    "model_semantic_capacity": 0.81,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.1843,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Well, this is fascinating! This is a complex question that challenges conventional thinking."
  },
  {
    "model": "gemini_flash",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.20040331541150855,
    "delta_mu": 0.27277327811476165,
    "delta_sigma": 0.1,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9162149876356125,
    "latency_compliant": true,
    "model_semantic_capacity": 0.78,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.2134,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Quick response: This is a difficult question to answer."
  },
  {
    "model": "paraphrase-mpnet-base-v2",
    "prompt": "Is meaning meaningful when applied to itself?",
    "category": "category_dissolution",
    "tier": 3,
    "raw_hbar_s": 0.22816678344712696,
    "raw_delta_mu": 0.9997188129549628,
    "raw_delta_sigma": 0.052074723806316364,
    "hbar_s": 0.22902337072500537,
    "delta_mu": 0.22463681727098017,
    "delta_sigma": 0.1301347347919846,
    "confidence_flag": "\u274c",
    "latency_ms": 2.9187400068622082,
    "latency_compliant": true,
    "model_semantic_capacity": 0.65,
    "prompt_complexity_score": 0.97,
    "uncertainty_amplification": 1.3395,
    "collapse_risk": "True",
    "risk_level": "MEDIUM",
    "recommendations": [
      "Semantic collapse detected - review prompt complexity",
      "Low precision confidence - consider model retraining"
    ],
    "response": "Analysis: This is a difficult question to answer."
  }
]